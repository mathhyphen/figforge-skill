We propose a novel neural architecture for multi-modal learning that combines vision and language processing. The model consists of three main stages: First, visual features are extracted using a pre-trained Vision Transformer (ViT), while text is processed through a BERT encoder. Second, these multi-modal features are fused using cross-attention mechanisms that allow bidirectional information flow between modalities. Third, the fused representations are passed through a multi-layer transformer decoder that generates task-specific outputs. We incorporate a novel gating mechanism that dynamically weights the contribution of each modality based on input characteristics. The architecture is trained end-to-end using a combination of contrastive loss for alignment and task-specific losses. Experimental results on VQA and image captioning benchmarks demonstrate significant improvements over baseline methods, with particularly strong performance on complex reasoning tasks that require deep cross-modal understanding.
